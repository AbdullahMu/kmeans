{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Clustering and K-Means\n",
    "\n",
    "_Instructor: Aymeric Flaisler_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/kYWumd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Understand basic unsupervised clustering problems\n",
    "- Format and preprocess data for cluster\n",
    "- Perform a K-Means Clustering Analysis\n",
    "- Evaluate clusters for fit \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Unsupervised learning](#unsupervised)\n",
    "- [Introduction to clustering](#intro)\n",
    "- [What is clustering?](#what)\n",
    "- [KNN review](#knn)\n",
    "- [Clustering algorithms](#algos)\n",
    "- [K-means clustering](#k-means)\n",
    "- [Refresher: Euclidean distance](#euclidean)\n",
    "- [K-Means step-by-step](#km-steps)\n",
    "- [K-Means: a visual example](#vis)\n",
    "- [K-Means caveats and pitfalls](#caveats)\n",
    "    - [Sensitive to outliers](#sensitive)\n",
    "    - [Sensitive to centroid initialization](#centroid-init)\n",
    "    - [How many K?](#how-many-k)\n",
    "- [Choosing K](#choose-k)\n",
    "- [A note on K-Means convergence](#converge)\n",
    "- [K-Means in sklearn](#kmeans-skl)\n",
    "    - [Visually verifying cluster labels](#verify)\n",
    "- [Metrics: inertia and the silhouette coefficient](#sil)\n",
    "- [Practice: use K-Means on the \"Isotopic Composition Plutonium Batches\" data](#pluto)\n",
    "    - [How does scaling affect fit?](#scaling)\n",
    "- [Conclusion: K-Means tradeoffs](#conclusion)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='unsupervised'></a>\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "---\n",
    " \n",
    "> **Supervised --> Classification** - create a model to predict which group a point belongs to.  \n",
    "> **Supervised --> Regression** - create a model to predict a metric.\n",
    "\n",
    "> **Unsupervised --> Clustering** - find groups that exist in the data already.\n",
    "\n",
    "Until now, we haven't talked much about unsupervised learning.  We use unsupervised methods when **we don't have labeled  data**. \n",
    "- There are no true targets to predict.\n",
    "\n",
    "We derive the likely categories from the structure in our data.\n",
    "\n",
    "| Pros | Cons |\n",
    "|---|---|\n",
    "| No labels | Difficult to evaluate correctness without subject matter expertise |\n",
    "| Few or no assumptions about data | Scaling / normalization often required |\n",
    "| Useful for subset / segmentation discovery | Can be difficult to visualize |\n",
    "| Great for broad insights | Extremely difficult to tune |\n",
    "| Many models avalable | No obvious choice in many cases |\n",
    "| Black magic | Considered \"unconventional\" and unreliable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction to clustering\n",
    "\n",
    "---\n",
    "\n",
    "### Helpful uses for clustering: \n",
    "   - Find items with **similar behavior** (users, products, voters, etc)\n",
    "   - Market **segmentation**\n",
    "   - Understand complex systems\n",
    "   - **Discover meaningful categories** for your data\n",
    "   - **Reduce the number of classes** by grouping (e.g. bourbons, scotches -> whiskeys)\n",
    "   - **Pre-processing**! Create labels for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Genetics\n",
    "![](https://snag.gy/TP2RA4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consumer Internet\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"https://snag.gy/EbLeqd.jpg\"></td>\n",
    "        <td><img src=\"https://snag.gy/xsNvK8.jpg\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Business\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td><img src=\"https://snag.gy/pDueQ2.jpg\" width=\"500\"></td>\n",
    "    <td>\n",
    "        <li>Identifying Demographics</li>\n",
    "        <li>Spending Patterns</li>\n",
    "        <li>Consumer Trends</li>\n",
    "        <li>Customer Characteristics</li>\n",
    "        <li>Recommender Systems</li>\n",
    "        <li>Taxonomy / Categorization</li>\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do we mean by \"labeled data\"?\n",
    "- Give me some examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='what'></a>\n",
    "## What is Clustering? \n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/BdfATE.jpg\" style=\"width: 500px\">\n",
    "\n",
    "Clustering is one of the most ubiquitous and widespread processes for assigning **discrete structure to data**. In clustering, we group observations in a dataset together such that the members of a group are **more similar to each other** than they are to members of other groups. There are a **wide variety of methods** and criteria to perform this task.\n",
    "\n",
    "**Properties of clustering procedures:**\n",
    "- No **\"true\" target (`y`)** / response to compare\n",
    "- We apply structure to data quantitatively based on **specific criteria**\n",
    "- Predictions of label are based **on the structure of the data**\n",
    "\n",
    "**For example:** your employer gives you a dataset of voter preferences from a local poll. They want you to figure out just exactly **how these voters are grouping based on their preferences**. The answer: clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='knn'></a>\n",
    "## KNN review\n",
    "\n",
    "---\n",
    "\n",
    "KNN is a supervised classification method.\n",
    "\n",
    "![](https://snag.gy/WPF4ZS.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Questions (in pair):** \n",
    "- Discuss the plus/cons of unsupervised learning. In which situations would you use it? \n",
    "- Recall how the KNN algorithm works for classification. How would you modify it to turn it into an unsupervised algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"algos\"></a>\n",
    "## Clustering algorithms \n",
    "\n",
    "---\n",
    "\n",
    "The are [many different algorithms](http://scikit-learn.org/stable/modules/clustering.html) that can perform clustering given a dataset:\n",
    "\n",
    "- **K-Means** (mean centroids)\n",
    "- **Heirarchical** (nested clusters by merging or splitting successively)\n",
    "- **DBSCAN** (density based)\n",
    "- **Affinity Propagation** (graph based approach to let points 'vote' on their preferred 'exemplar')\n",
    "- **Mean Shift** (can find number of clusters)\n",
    "- **Spectral Clustering**\n",
    "- **Agglomerative Clustering** (suite of algorithms all based on applying the same criteria/characteristics of one cluster to others)\n",
    "\n",
    "\n",
    "\n",
    "Today we're going to look only at one of the algorithms: **k-means.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='k-means'></a>\n",
    "## K-Means clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### K-Means is the most popular clustering algorithm\n",
    "\n",
    "K-means is one of the easier methods to understand and other clustering techniques use some of the same assumptions that K-Means relies on.\n",
    "\n",
    "- **K** is the number of clusters.\n",
    "- **Means** refers to the mean points of the K clusters.\n",
    "\n",
    "The number of clusters $k$ is chosen in advance. The goal is to partition the data into set **such that the total sum of squared distances** from each point to the mean point of the cluster is minimized.\n",
    "\n",
    "The algorithm takes your entire dataset and iterates over its features and observations to determine clusters based around center points. These center points are known as **centroids**. \n",
    "\n",
    "**Question:** What can you say about the loss function of the K-Means algorithm? What are the 2 most important consequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**K-means iterative fitting:**\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Create your clusters. Assign each point to the nearest centroid. \n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "> **Note:** Unfortunately there is no formula to determine the absolute best number of $k$ clusters. Unsupervised learning is inherently subjective! We can, however, choose the \"best\" $k$ based on predetermined criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='euclidean'></a>\n",
    "## Refresher: Euclidean distance\n",
    "\n",
    "---\n",
    "\n",
    "### $$ d(x_1, x_2) = \\sqrt{\\sum_{i=1}^N (x_{1i} - x_{2i})^2} $$\n",
    "\n",
    "**For example, take two points:**\n",
    "\n",
    "- $x1 = (2, -1, 3)$\n",
    "- $x2 = (-2, 2, 3)$\n",
    "\n",
    "**The Euclidean distance between these two points is:**\n",
    "\n",
    "### $$\\begin{aligned}\n",
    "d(x1, x2) &= \\sqrt{ (2 - (-2))^2 + ((-1) - 2)^2 +(3 - 3)^2} \\\\\n",
    "d(x1, x2) &= \\sqrt{25} \\\\\n",
    "d(x1, x2) &= 5 \n",
    "\\end{aligned}$$\n",
    "\n",
    "**Using sklearn:**\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "X = np.array([[2, -1], [-2, 2]])\n",
    "print(euclidean_distances(X))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='km-steps'></a>\n",
    "## K-Means step-by-step\n",
    "\n",
    "---\n",
    "\n",
    "<table width=\"500\" cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/7haoS3.jpg\" style=\"width: 150px\"></td>\n",
    "   <td style=\"vertical-align: top; width: 400px;\"><br><b>Step 1.</b><br>We have data in a N-Dimensional feature space (2D for example).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/DaIVgk.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><br><b>Step 2.</b><br>Intialize K centroid (2 here).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/DaIVgk.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 3.</b><br>Assign points to *closest* cluster based on _euclidean distance_.<br><br>$\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$\n",
    "\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/NY1EeT.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 4.</b><br>Calculate mean of points assigned to centroid (2 here).  Update new centroid positions to mean (ie: geometric center).<br><br>$new\\ centroid\\ position= \\bar{x}, \\bar{y}$\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/tSfDZs.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 5.</b><br>Repeat step 3-4, updating class membership based on centroid distance.\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/BbIicn.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Fin.</b><br>Convergence is met once all points no longer change to a new class (defined by closest centroid distance).\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='vis'></a>\n",
    "## K-Means: a visual example\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/5hFXUA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='caveats'></a>\n",
    "## A few K-Means caveats...\n",
    "\n",
    "---\n",
    "\n",
    "Nothing's perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='sensitive'></a>\n",
    "### K-Means is sensitive to outliers\n",
    "\n",
    "![](https://snag.gy/WFNMQY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='centroid-init'></a>\n",
    "### K-Means is sensitive to centroid initialization\n",
    "\n",
    "![](https://snag.gy/5sigCD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='how-many-k'></a>\n",
    "### How many K?\n",
    "\n",
    "Sometimes it's obvious, sometimes it's not!  What do you think?\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/4rU39.png\"><br>1</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/gq28F.png\"><br>2</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"https://snag.gy/cWPgno.jpg\"><br>3</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='choose-k'></a>\n",
    "## Choosing K\n",
    "\n",
    "---\n",
    "\n",
    "There are different methods of initializing centroids. For instance:\n",
    "\n",
    "- Randomly\n",
    "- Manually\n",
    "- Special KMeans++ method in Sklearn (_This initializes the centroids to be generally distant from each other_)\n",
    "\n",
    "**Depending on your problem, you may find some of these are better than others.**\n",
    "\n",
    "> **Note:** Manual is recommended if you know your data well enough to see the clusters without much help, but rarely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='converge'></a>\n",
    "## A note on K-Means convergence\n",
    "\n",
    "---\n",
    "\n",
    "In general, k-means will converge to a solution and return a partition of k clusters, even if no natural clusters exist in the data.  **It's entirely possible – in fact, *common* – that the clusters do not mean anything at all. **\n",
    "\n",
    "**Knowing your domain and dataset is essential. Evaluating the clusters visually is a must (if possible).**\n",
    "\n",
    "> _\"Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='kmeans++' parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.\"_ [sklearn Clustering Guide](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "![](http://www.datamilk.com/kmeans_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://snag.gy/YUt5RO.jpg\" style=\"float: left; margin-right: 25px; width: 250px\">\n",
    "\n",
    "## Pair questions:\n",
    "\n",
    "- Explain on paper how the k-means algorithm works\n",
    "- What problems do you think arise during \"clustering\"?\n",
    "- What about the dimensions? Is it a problem for this algorithm? Explain with your own words\n",
    "- What kind of data are we talking about? In the simple form of Kmeans, discuss its sensitivity to outliers.\n",
    "- For two runs of K-Mean clustering is it expected to always get same clustering results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kmeans-skl'></a>\n",
    "## K-Means in sklearn\n",
    "\n",
    "---\n",
    "\n",
    "Below we will implement K-Means using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = make_blobs(\n",
    "    n_samples    =  200, \n",
    "    centers      =  3, \n",
    "    n_features   =  2,\n",
    "    random_state =  0\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X, columns=['x', 'y'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After we fit our data, we can get our predicted labels from `model.labels_` and the center points`model.cluster_centers_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "predicted ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# get centroids \n",
    "centroids = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# add predicted labels to dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='verify'></a>\n",
    "### Visually verifying cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='verify'></a>\n",
    "### Elbow method to find K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion: K-Means tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "**K-Means:**\n",
    "- Unsupervised clustering model\n",
    "- Similar to KNN (but for “clustering”)\n",
    "- Iteratively finds labels given K\n",
    "- Easy to implement in sklearn\n",
    "- Sensitive to shape, scale of data\n",
    "- Optimal K hard to evaluate\n",
    "\n",
    "---\n",
    "\n",
    "| Strengths | Weaknesses |\n",
    "| -- | -- |\n",
    "| K-Means is popular because it's simple and computationally efficient. | However, K-Means is highly scale dependent and isn't suitable for data of varying shapes and densities. |\n",
    "| Easy to see results / intuitive. | Evaluating results is more subjective, requiring much more human evaluation than trusted metrics. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- Andrew Moore's CS class at Carnegie Mellon contains good static visualization, step-by-step. His slide deck is online here: http://www.cs.cmu.edu/~cga/ai-course/kmeans.pdf. He also links to more of his tutorials on the first page. \n",
    "- [Sci-Kit Learn Clustering Overview](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "- [SKLearn K-Means Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "- [SKLearn Clustering Code - See _k_init__ for explanation of k++ centroid selection](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/k_means_.py#L769)\n",
    "- [Clustering Tutorial](http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/)\n",
    "- [Wikipedia's Deep Dive on Clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "\n",
    "\n",
    "**Some helpful stackexchange questions:**\n",
    "- http://stats.stackexchange.com/questions/40613/why-dont-dummy-variables-have-the-continuous-adjacent-category-problem-in-clust\n",
    "- http://stats.stackexchange.com/questions/174556/k-means-clustering-with-dummy-variables\n",
    "- http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
